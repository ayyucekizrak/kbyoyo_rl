{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "Bu Notebook OpenAI Gym'de Ice World'de value iteration ornegi icermektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Stuff\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "max_iter = 1000\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    "Kis geldi ve arkadaslarinizla firizbi oynuyorsunuz. Yanlislikla frizbiyi donmus bir gölun uzerine firlattiniz. Gidip firizbiyi almaniz gerekmekte. Fakat, donmus golun bazi yerlerinde delikler var ve delige dusebilirsiniz! Ayni zamanda yer kaygan oldugu icin gitmek istediginiz yerden baska yere gidebilirsiniz. Bu problemde göl bir _grid world_ olarak gosterilmektedir. Bu bir _discreate state_ ve _discrete action_ problemidir.\n",
    "\n",
    "SFFF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(S: _starting point_, guvenli)  \n",
    "FHFH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(F: _frozen surface_, guvenli)  \n",
    "FFFH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(H: _hole_, cehennem)  \n",
    "HFFG&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(G: _goal_, firizbinin bulundugu yer)  \n",
    "\n",
    "OpenAI Gym Notebookun'da asagidaki kodu kullanmistik.\n",
    "\n",
    "```python\n",
    "observation, reward, done, info = env.step(env.action_space.sample())\n",
    "```\n",
    "\n",
    "Artik rasgele aksiyonlar yerine, daha \"akilli\" secimler yapmamiz gerekmekte. Bunun icin her noktanin valuesunu iceren bir tablo olusturmamiz gerekiyor. Bu value tablosunu value iteration algoritmasi ile guncelleyecegiz edecegiz. Son olarak gol uzerinde her noktada valuesu en yuksek olan bir sonraki noktaya hareket edecegiz.\n",
    "\n",
    "Value Iteration:  \n",
    "$$V(s) \\leftarrow max_a \\sum_{s', r} P(s',r|s,a)[r(s) + \\gamma V(s')]$$\n",
    "\n",
    "$$P(s',r|s,a) = \\frac{1}{3}$$\n",
    "\n",
    "$$\n",
    "r(s) = \\left\\{ \\begin{array}{rl}\n",
    "1 &\\mbox{ if $s=G$} \\\\\n",
    "0 &\\mbox{ otherwise}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "OpenAI Gym'de Environment objelerinin ``P`` sembolu bizlere butun olasi siradaki stateleri, odulu ve bitip bitmedigini vermektedir. Ornek: \n",
    "\n",
    "```python\n",
    "for transition, nextstate, reward, done in env.P[s][a]:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value tablosunu olusturalim\n",
    "env = gym.make('FrozenLake-v0')\n",
    "values = np.zeros(env.nS)\n",
    "policy = np.zeros(env.nS)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    delta = 0.0\n",
    "    # Her state icin\n",
    "    for s in range(env.nS):\n",
    "        v = values[s]\n",
    "        action_values = np.zeros(env.nA)\n",
    "        \n",
    "        # Her action icin\n",
    "        for a in range(env.nA):\n",
    "            # Her (s,a) -> s' icin\n",
    "            for transition, nextstate, reward, done in env.P[s][a]:\n",
    "                action_values[a] += transition * (reward + gamma*values[nextstate])\n",
    "\n",
    "        new_value = np.max(action_values)\n",
    "        delta = np.max((delta, np.abs(new_value - v)))\n",
    "        values[s] = new_value\n",
    "        policy[s] = np.argmax(action_values)\n",
    "        \n",
    "    if delta < 1e-8:\n",
    "        print(\"Converged at: {}\".format(i+1))\n",
    "        break\n",
    "        \n",
    "policy = policy.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our grid world\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our values\n",
    "print(policy)\n",
    "img = plt.imshow(values.reshape(4,4))\n",
    "plt.colorbar(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "env.render()\n",
    "\n",
    "while True:\n",
    "    action = policy[s]   \n",
    "    s, r, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
